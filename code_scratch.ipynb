{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd168f7",
   "metadata": {},
   "source": [
    "# MVTEC: Anomaly Detection using PatchCore (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d98e0",
   "metadata": {},
   "source": [
    "This notebook is implementation of Patchcore paper using PyTorch from scratch.\n",
    "\n",
    "PatchCore is a state-of-the-art image anomaly detection model for the MVTec dataset, according to the Papers with Code website. It utilizes a pre-trained ResNet50 model to create a memory bank of good images. This memory bank is used to check the similarity between images of the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b49af",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecd171",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54995698",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b770c67",
   "metadata": {},
   "source": [
    "Load a pretrained Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e117b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class resnet_feature_extractor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"This class extracts the feature maps from a pretrained Resnet model.\"\"\"\n",
    "        super(resnet_feature_extractor, self).__init__()\n",
    "        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        \n",
    "        # Hook to extract feature maps\n",
    "        def hook(module, input, output) -> None:\n",
    "            \"\"\"This hook saves the extracted feature map on self.featured.\"\"\"\n",
    "            self.features.append(output)\n",
    "\n",
    "        self.model.layer2[-1].register_forward_hook(hook)            \n",
    "        self.model.layer3[-1].register_forward_hook(hook) \n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        self.features = []\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(input)\n",
    "\n",
    "        self.avg = torch.nn.AvgPool2d(3, stride=1)\n",
    "        fmap_size = self.features[0].shape[-2]         # Feature map sizes h, w\n",
    "        self.resize = torch.nn.AdaptiveAvgPool2d(fmap_size)\n",
    "\n",
    "        resized_maps = [self.resize(self.avg(fmap)) for fmap in self.features]\n",
    "        patch = torch.cat(resized_maps, 1)            # Merge the resized feature maps\n",
    "        patch = patch.reshape(patch.shape[1], -1).T   # Craete a column tensor\n",
    "\n",
    "        return patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbd6eb",
   "metadata": {},
   "source": [
    "Check feature shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82783cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "backbone = resnet_feature_extractor().cuda()\n",
    "\n",
    "sample_img_path = '/kaggle/input/mvtec-ad/hazelnut/test/hole/000.png'\n",
    "image = Image.open(sample_img_path)\n",
    "image = transform(image).unsqueeze(0).cuda()\n",
    "\n",
    "feature = backbone(image)\n",
    "\n",
    "print(backbone.features[0].shape)\n",
    "print(backbone.features[1].shape)\n",
    "\n",
    "print(feature.shape)\n",
    "\n",
    "plt.imshow(image[0].cpu().permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c18c6",
   "metadata": {},
   "source": [
    "Create memory bank from GOOD images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c2b6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "memory_bank = []\n",
    "\n",
    "folder_path = Path(r'/kaggle/input/mvtec-ad/hazelnut/train/good')\n",
    "\n",
    "for pth in tqdm(folder_path.iterdir(),leave=False):\n",
    "    # print(pth)\n",
    "    with torch.no_grad():\n",
    "        data = transform(Image.open(pth)).cuda().unsqueeze(0)\n",
    "        features = backbone(data)\n",
    "        # print(features.shape)\n",
    "        memory_bank.append(features.cpu().detach())\n",
    "\n",
    "print(len(memory_bank))\n",
    "print(memory_bank[0].shape)\n",
    "memory_bank = torch.cat(memory_bank,dim=0).cuda()\n",
    "memory_bank.shape # 784x391 = 306544"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575de309",
   "metadata": {},
   "source": [
    "Random Sampling - Select 10% of total patches to avoid long inference and computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfafee8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "selected_indices = np.random.choice(len(memory_bank), size=len(memory_bank)//10, replace=False)\n",
    "memory_bank = memory_bank[selected_indices]\n",
    "memory_bank.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1fda7",
   "metadata": {},
   "source": [
    "For GOOD images [K nearsest neighbours]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ded4f",
   "metadata": {},
   "source": [
    "Distance scores for good images, to calculate threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b85b97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_score_good = []\n",
    "folder_path = Path(r'/kaggle/input/mvtec-ad/hazelnut/train/good')\n",
    "\n",
    "for pth in tqdm(folder_path.iterdir(),leave=False):\n",
    "    data = transform(Image.open(pth)).cuda().unsqueeze(0)\n",
    "    # print(data.shape)\n",
    "    with torch.no_grad():\n",
    "        features = backbone(data)\n",
    "        # print(features.shape)\n",
    "    distances = torch.cdist(features, memory_bank, p=2.0)\n",
    "    # print(distances.shape)\n",
    "    dist_score, dist_score_idxs = torch.min(distances, dim=1) \n",
    "    # print(dist_score[:10], dist_score_idxs[:10])\n",
    "    s_star = torch.max(dist_score)\n",
    "    # print(s_star)\n",
    "    segm_map = dist_score.view(1, 1, 28, 28) \n",
    "    # print(segm_map.shape)\n",
    "\n",
    "    y_score_good.append(s_star.cpu().numpy())\n",
    "    # break\n",
    "\n",
    "y_score_good[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c602e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_score_good[:5]\n",
    "# image_np = segm_map.squeeze().cpu() # Remove batch & channel dimensions\n",
    "\n",
    "# # Plot the image\n",
    "# plt.imshow(image_np, cmap='gray')\n",
    "# plt.title(\"28x28 Image\")\n",
    "# plt.axis(\"off\")  # Hide axes\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e87df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(y_score_good))\n",
    "print(np.std(y_score_good))\n",
    "\n",
    "best_threshold = np.mean(y_score_good) + 3 * np.std(y_score_good)\n",
    "print(f\"Threshold: {best_threshold}\")\n",
    "\n",
    "plt.hist(y_score_good, bins=50)\n",
    "plt.vlines(x=best_threshold, ymin=0, ymax=30, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b702b2fc",
   "metadata": {},
   "source": [
    "For BAD Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801fdb9e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_score = []\n",
    "y_true = []\n",
    "\n",
    "for classes in ['crack', 'cut', 'good', 'hole', 'print']:\n",
    "    folder_path_test = Path(f\"/kaggle/input/mvtec-ad/hazelnut/test/{classes}\")\n",
    "    \n",
    "    for pth in tqdm(folder_path_test.iterdir(),leave=False):\n",
    "        # print(pth)\n",
    "        class_label = pth.parts[-2]\n",
    "        # print(class_label)\n",
    "        with torch.no_grad():\n",
    "            test_image = transform(Image.open(pth)).cuda().unsqueeze(0)\n",
    "            features = backbone(test_image)\n",
    "\n",
    "        distances = torch.cdist(features, memory_bank, p=2.0)\n",
    "        dist_score, dist_score_idxs = torch.min(distances, dim=1) \n",
    "        s_star = torch.max(dist_score)\n",
    "        segm_map = dist_score.view(1, 1, 28, 28) \n",
    "\n",
    "        y_score.append(s_star.cpu().numpy())\n",
    "        y_true.append(0 if class_label=='good' else 1)  # 0 -> GOOD, 1 -> BAD\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf60c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_score[40:45], y_true[40:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab1835",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# plotting the y_score values which belong to 'BAD' class\n",
    "\n",
    "y_score_bad = [score for score,true in zip(y_score, y_true) if true==1]\n",
    "plt.hist(y_score_bad,bins=50)\n",
    "plt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91842b76",
   "metadata": {},
   "source": [
    "Visualize one anomaly image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6672767",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_image = transform(Image.open(r'/kaggle/input/mvtec-ad/hazelnut/test/cut/000.png')).cuda().unsqueeze(0)\n",
    "features = backbone(test_image)\n",
    "\n",
    "distances = torch.cdist(features, memory_bank, p=2.0)\n",
    "dist_score, dist_score_idxs = torch.min(distances, dim=1) \n",
    "s_star = torch.max(dist_score)\n",
    "segm_map = dist_score.view(1, 1, 28, 28)\n",
    "\n",
    "# Upscale by bi-linaer interpolation to match the original input resolution\n",
    "segm_map = torch.nn.functional.interpolate(\n",
    "                segm_map,\n",
    "                size=(224, 224),\n",
    "                mode='bilinear'\n",
    "                )\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(segm_map.cpu().squeeze(), cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d79694",
   "metadata": {},
   "source": [
    "Evaluation Matrices - best threshold calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973647d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "# Calculate AUC-ROC score\n",
    "auc_roc_score = roc_auc_score(y_true, y_score)\n",
    "print(\"AUC-ROC Score:\", auc_roc_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "print(\"fpr, tpr, thresholds: \", fpr, tpr, thresholds)\n",
    "\n",
    "f1_scores = [f1_score(y_true, y_score >= threshold) for threshold in thresholds]\n",
    "print(\"f1_scores:\", f1_scores)\n",
    "\n",
    "# Select the best threshold based on F1 score\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f'best_threshold = {best_threshold}')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, (y_score >= best_threshold).astype(int))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['GOOD', 'BAD'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becb1ab",
   "metadata": {},
   "source": [
    "Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44190d40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2, time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "backbone.eval()\n",
    "class_label = ['GOOD', 'BAD']\n",
    "test_path = Path('/kaggle/input/mvtec-ad/hazelnut/test')\n",
    "\n",
    "for path in test_path.glob('*/*.png'): \n",
    "\n",
    "    fault_type = path.parts[-2]\n",
    "    if fault_type in ['hole']:  # change defect type - crack, cut, hole, print, good\n",
    "        \n",
    "        test_image = transform(Image.open(path)).cuda().unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = backbone(test_image)\n",
    "        # Forward pass\n",
    "        distances = torch.cdist(features, memory_bank, p=2.0)\n",
    "        dist_score, dist_score_idxs = torch.min(distances, dim=1) \n",
    "        s_star = torch.max(dist_score)\n",
    "        segm_map = dist_score.view(1, 1, 28, 28) \n",
    "        # Upscale by bi-linear interpolation to match the original input resolution\n",
    "        segm_map = torch.nn.functional.interpolate(\n",
    "                    segm_map,\n",
    "                    size=(224, 224),\n",
    "                    mode='bilinear'\n",
    "                ).cpu().squeeze().numpy()\n",
    "        \n",
    "        y_score_image = s_star.cpu().numpy()  \n",
    "        y_pred_image = 1*(y_score_image>=best_threshold)\n",
    "        \n",
    "        plt.figure(figsize=(12,3))\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(test_image.squeeze().permute(1,2,0).cpu().numpy())\n",
    "        plt.title(f'fault type: {fault_type}')\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        heat_map = segm_map\n",
    "        plt.imshow(heat_map, cmap='jet', vmin=best_threshold, vmax = best_threshold * 2) \n",
    "        plt.title(f'Anomaly score: {y_score_image:0.2f} | {class_label[y_pred_image]}')\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow((heat_map > best_threshold ), cmap='gray')  #\n",
    "        plt.title(f'segmentation map')\n",
    "        \n",
    "        plt.show()\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ceb2d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848f34e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
